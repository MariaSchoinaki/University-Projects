{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea8c9f7-cef0-4586-b0df-2bf4cfb0a576",
   "metadata": {},
   "source": [
    "# SemEval 2025 Task 9: Food Hazard Detection\n",
    "Student: **Maria Schoinaki**\n",
    "\n",
    "## **Introduction**\n",
    "The **SemEval 2025 Task 9: Food Hazard Detection Challenge** focuses on classifying food incident reports by predicting the **type of hazard** and **product** mentioned in each report.\n",
    "\n",
    "This task aims to support automated food safety monitoring by analyzing **titles** and **full texts** of recall reports. The challenge is divided into **two sub-tasks**:\n",
    "\n",
    "- **ST1: Food Hazard Classification**  \n",
    "   - Predicts **hazard-category** (e.g., biological, chemical)  \n",
    "   - Predicts **product-category** (e.g., dairy, seafood, beverages)  \n",
    "\n",
    "- **ST2: Food Hazard and Product Vector Detection**  \n",
    "   - Predicts **exact hazard** (e.g., Salmonella, Listeria)  \n",
    "   - Predicts **exact product** (e.g., Ice Cream, Chicken, Cake)  \n",
    "\n",
    "Since food safety risks are critical, **explainability** is an important factor in this challenge. The results will help **food agencies and regulators** track hazardous food items efficiently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd4845-5212-4a98-9c26-c598b8780352",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "To procced running the code, we need to install the following dependencies:\n",
    "1. transformers[torch]\n",
    "2. imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf16609b-8a60-45df-9b48-4186b535b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# install dependencies:\n",
    "!pip install transformers[torch]\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2649c-78fd-49f4-8a9f-f76ade9593d7",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "To participate in this challenge, we need three datasets:\n",
    "1. **Training Data (Labeled)**\n",
    "2. **Validation Data (Unlabeled)**\n",
    "3. **Testing Data (Unlabeled)**\n",
    "\n",
    "We download the datasets using `wget`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50741b44-45ac-4098-a793-dc12911fd0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-13 20:28:16--  https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 12866710 (12M) [text/plain]\n",
      "Saving to: ‘incidents_train.csv’\n",
      "\n",
      "100%[======================================>] 12,866,710  --.-K/s   in 0.04s   \n",
      "\n",
      "2025-02-13 20:28:16 (290 MB/s) - ‘incidents_train.csv’ saved [12866710/12866710]\n",
      "\n",
      "--2025-02-13 20:28:16--  https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_valid.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 1369261 (1.3M) [text/plain]\n",
      "Saving to: ‘incidents_valid.csv’\n",
      "\n",
      "100%[======================================>] 1,369,261   --.-K/s   in 0.01s   \n",
      "\n",
      "2025-02-13 20:28:16 (93.4 MB/s) - ‘incidents_valid.csv’ saved [1369261/1369261]\n",
      "\n",
      "--2025-02-13 20:28:16--  https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_test.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 2538599 (2.4M) [text/plain]\n",
      "Saving to: ‘incidents_test.csv’\n",
      "\n",
      "100%[======================================>] 2,538,599   --.-K/s   in 0.02s   \n",
      "\n",
      "2025-02-13 20:28:16 (130 MB/s) - ‘incidents_test.csv’ saved [2538599/2538599]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download training data (labeled):\n",
    "!wget https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_train.csv\n",
    "\n",
    "# download validation data (unlabeled):\n",
    "!wget https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_valid.csv\n",
    "\n",
    "# download testing data (unlabeled):\n",
    "!wget https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4fc82-53c1-4487-a6b2-cf4367cbe6f9",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "To build a **food hazard detection system**, we require various Python libraries for **data processing, model training, evaluation, and explainability**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540f9ba6-a90d-4b14-9093-7b1df897357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 20:28:30.818706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-13 20:28:30.847405: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-13 20:28:30.847453: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 20:28:30.865169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 20:28:32.033935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Transformers is only compatible with Keras 2, but you have explicitly set `TF_USE_LEGACY_KERAS` to `0`. This may result in unexpected behaviour or errors if Keras 3 objects are passed to Transformers models.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c8b27-97ef-48b2-9577-6e0bf6ae3052",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "To train and evaluate the **food hazard detection model**, we need to **load, extract, and structure** the dataset properly. This section explains how we load the dataset and separate it into **features (X)** and **labels (y)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Overview\n",
    "The dataset consists of **food hazard reports**, where each record contains:\n",
    "- A **title** (short description of the incident)\n",
    "- A **text** (detailed description of the incident)\n",
    "- Four labels for classification:\n",
    "  1. `hazard-category` (Broad category of hazard: Biological, Chemical, etc.)\n",
    "  2. `product-category` (General food type: Dairy, Seafood, etc.)\n",
    "  3. `hazard` (Specific hazard: Salmonella, E. Coli, etc.)\n",
    "  4. `product` (Specific product: Milk, Ice Cream, Chicken, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### Loading the Dataset\n",
    "We load three separate CSV files into **Pandas DataFrames**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97706ee-5941-4089-8c16-7fefecf31d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_data = pd.read_csv(\"incidents_train.csv\", index_col=0)\n",
    "val_data = pd.read_csv(\"incidents_valid.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"incidents_test.csv\", index_col=0)\n",
    "\n",
    "# Extract Inputs and Labels\n",
    "X_train, y_train = train_data[[\"title\", \"text\"]], train_data[[\"hazard-category\", \"product-category\", \"hazard\", \"product\"]]\n",
    "X_val, y_val = val_data[[\"title\", \"text\"]], val_data[[\"hazard-category\", \"product-category\", \"hazard\", \"product\"]]\n",
    "X_test, y_test = test_data[[\"title\", \"text\"]], test_data[[\"hazard-category\", \"product-category\", \"hazard\", \"product\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588c48a-01c5-4acd-9556-2a7a290242c4",
   "metadata": {},
   "source": [
    "## Ensuring Reproducibility\n",
    "\n",
    "To guarantee **consistent results** across multiple training runs, we must **set a fixed seed** for all libraries. Additionally, we check if a **GPU (CUDA)** is available to accelerate training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6316a5b-64e0-4b48-9546-88b2c3da2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Ensure Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75f047-a03e-4417-afe5-4730640b241e",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Before feeding text data into a **deep learning model**, we must **preprocess** it to improve model performance. This involves **cleaning** and **standardizing** the text.\n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing Steps\n",
    "We define a **simple yet effective preprocessing function** that performs:\n",
    "1. **Converting to lowercase** → Ensures uniform text representation.\n",
    "2. **Stripping extra spaces** → Removes unnecessary whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f7d888-4d84-421a-aeaa-404c82825498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "X_train = X_train.applymap(preprocess_text)\n",
    "X_val = X_val.applymap(preprocess_text)\n",
    "X_test = X_test.applymap(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31e96e-c1e2-4095-a1b8-5fdc3019de19",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "In order to train a **machine learning model**, categorical labels must be **converted into numerical values**. This step ensures that the model can process and understand class labels effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Label Encoding Important?\n",
    "- **Converts categorical labels into numbers** → Machine learning models require numerical inputs.\n",
    "- **Ensures consistency across datasets** → Labels are encoded the same way in training, validation, and test sets.\n",
    "- **Handles unseen labels in the test set** → Prevents errors due to labels not seen during training.\n",
    "\n",
    "---\n",
    "\n",
    "### Label Encoding Steps\n",
    "We use **`sklearn.preprocessing.LabelEncoder`** to **convert class labels into numerical values**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72d5e14-e671-4c18-8787-a98750067613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "label_encoders = {}\n",
    "for col in [\"hazard-category\", \"product-category\", \"hazard\", \"product\"]:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    all_labels = pd.concat([y_train[col], y_val[col]], axis=0).astype(str)\n",
    "    label_encoders[col].fit(all_labels)\n",
    "\n",
    "    y_train[col] = label_encoders[col].transform(y_train[col].astype(str))\n",
    "    y_val[col] = label_encoders[col].transform(y_val[col].astype(str))\n",
    "\n",
    "    # Replace -1 in y_test with the Most Frequent Class\n",
    "    most_frequent_label = y_train[col].value_counts().idxmax()\n",
    "    y_test[col] = y_test[col].apply(\n",
    "        lambda x: label_encoders[col].transform([str(x)])[0] if str(x) in label_encoders[col].classes_ else most_frequent_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab5dcd-8e5e-4ede-ad3c-b17d4f74f668",
   "metadata": {},
   "source": [
    "## Compute Class Weights\n",
    "\n",
    "To handle class imbalance, we compute class weights for each classification task. \n",
    "This ensures that the model does not become biased towards more frequent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf49a72-71a0-419f-846c-1243aab3a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Class Weights\n",
    "class_weights = {}\n",
    "for col in [\"hazard-category\", \"product-category\", \"hazard\", \"product\"]:\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_train[col]), y=y_train[col])\n",
    "    class_weights[col] = torch.tensor(weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6999b97-1059-4cf5-b60a-b0dc7546b3e4",
   "metadata": {},
   "source": [
    "## Apply SMOTE for Balancing\n",
    "\n",
    "To handle class imbalance, we apply **Synthetic Minority Over-sampling Technique (SMOTE)** \n",
    "on the product classification task to generate synthetic samples for underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d117e6-f016-4867-9ac1-a4b70fdac357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE failed: Expected n_neighbors <= n_samples_fit, but n_neighbors = 3, n_samples_fit = 1, n_samples = 1. Proceeding without SMOTE.\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE for Balancing\n",
    "vectorizer = TfidfVectorizer(max_features=7000, stop_words=\"english\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train[\"text\"])\n",
    "\n",
    "try:\n",
    "    if len(set(y_train[\"product\"])) > 1:\n",
    "        smote = SMOTE(k_neighbors=2)\n",
    "        X_train_resampled, y_train_product_resampled = smote.fit_resample(X_train_tfidf, y_train[\"product\"])\n",
    "        y_train[\"product\"] = y_train_product_resampled\n",
    "except ValueError as e:\n",
    "    print(f\"SMOTE failed: {e}. Proceeding without SMOTE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfec6a0-ad3d-4432-8081-111727f7a991",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "To tokenize our text data, we use the **RoBERTa tokenizer** from Hugging Face's `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa6d323-b487-4e7d-89f1-bbf96997ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576f64f-d7fb-43ed-bb01-f4dc8e2cbde3",
   "metadata": {},
   "source": [
    "## Tokenization Function\n",
    "\n",
    "To prepare our text data for the **RoBERTa** model, we define a tokenization function that:\n",
    "- **Tokenizes both the title and text fields**\n",
    "- **Truncates** long sequences to a maximum of 256 tokens\n",
    "- **Pads** shorter sequences to ensure uniform input size\n",
    "- Returns **PyTorch tensors** for model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a557e1-1a1b-46ac-842e-17695db5fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Function\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        texts[\"title\"].tolist(),\n",
    "        texts[\"text\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_train = tokenize_data(X_train)\n",
    "tokenized_val = tokenize_data(X_val)\n",
    "tokenized_test = tokenize_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0d0d7-8546-4835-8031-6bbda0d93c03",
   "metadata": {},
   "source": [
    "## Convert to PyTorch Dataset\n",
    "\n",
    "To facilitate model training, we define a custom PyTorch `Dataset` class that allows efficient data loading.\n",
    "\n",
    "### **Class: `FoodHazardDataset`**\n",
    "This class prepares the tokenized inputs and their corresponding labels for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c189f76a-59b0-4a4a-9d52-f2d275e00153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch Dataset\n",
    "class FoodHazardDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73001775-ca89-4c6d-a600-ac880aa8eb52",
   "metadata": {},
   "source": [
    "## **Prepare Datasets**\n",
    "\n",
    "We structure the data into **PyTorch datasets** for training, validation, and testing. Each dataset contains **tokenized inputs** and **encoded labels** for the four prediction targets:\n",
    "\n",
    "1. **hazard-category** (ST1)\n",
    "2. **product-category** (ST1)\n",
    "3. **hazard** (ST2)\n",
    "4. **product** (ST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2def6eb-571b-4305-8946-950e8cab32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Datasets\n",
    "datasets = {}\n",
    "for col in [\"hazard-category\", \"product-category\", \"hazard\", \"product\"]:\n",
    "    datasets[col] = {\n",
    "        \"train\": FoodHazardDataset(tokenized_train, y_train[col].tolist()),\n",
    "        \"val\": FoodHazardDataset(tokenized_val, y_val[col].tolist()),\n",
    "        \"test\": FoodHazardDataset(tokenized_test, y_test[col].tolist())\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b2dc3-ea46-4f67-853a-3088f63e82a6",
   "metadata": {},
   "source": [
    "## Define RoBERTa Models\n",
    "\n",
    "We define separate **RoBERTa-based classification models** for each of the four classification tasks:\n",
    "- **Hazard Category**\n",
    "- **Product Category**\n",
    "- **Hazard**\n",
    "- **Product**\n",
    "\n",
    "Each model is initialized with **RoBERTa-Base** and adjusted for the number of unique labels in each classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a24f30d-7994-4459-bd05-1cbae3e3e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define Models\n",
    "models = {}\n",
    "for col in [\"hazard-category\", \"product-category\", \"hazard\", \"product\"]:\n",
    "    num_classes = len(label_encoders[col].classes_)\n",
    "    models[col] = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae0b17-e427-45ce-9ecb-3f3026568941",
   "metadata": {},
   "source": [
    "## Fine-Tuned Training Arguments\n",
    "\n",
    "We configure **optimized training parameters** to ensure the best balance between performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b1e35a-f867-42c1-822c-a58787d64fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuned Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.03,  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe1254-a1a6-4ee8-b264-5fd5bbf75b9e",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We train separate **RoBERTa** models for each classification task:\n",
    "- **Hazard Category**\n",
    "- **Product Category**\n",
    "- **Hazard Type**\n",
    "- **Product Type**\n",
    "\n",
    "Each model is fine-tuned using **gradient accumulation**, **learning rate scheduling**, and **early stopping** to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bea91836-1c09-4308-88cf-987b843447b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for: hazard-category\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 636/1590 01:49 < 02:44, 5.81 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.307579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.227359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.259463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.268308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for: product-category\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='795' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 795/1590 02:25 < 02:26, 5.44 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.368200</td>\n",
       "      <td>1.289274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.973900</td>\n",
       "      <td>0.926525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>0.824088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.513200</td>\n",
       "      <td>0.836142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.467700</td>\n",
       "      <td>0.912111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for: hazard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 04:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.479500</td>\n",
       "      <td>1.658876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.397400</td>\n",
       "      <td>1.191980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.189900</td>\n",
       "      <td>1.043220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>0.911452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.769400</td>\n",
       "      <td>0.844130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.637700</td>\n",
       "      <td>0.815853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.751730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.768021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.748307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for: product\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1590/1590 04:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.696900</td>\n",
       "      <td>6.306212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.908000</td>\n",
       "      <td>5.679848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.467900</td>\n",
       "      <td>5.158534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.870900</td>\n",
       "      <td>4.805787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.612900</td>\n",
       "      <td>4.505324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.119600</td>\n",
       "      <td>4.264974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.752900</td>\n",
       "      <td>4.093976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.538900</td>\n",
       "      <td>3.935555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.251100</td>\n",
       "      <td>3.820834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.075900</td>\n",
       "      <td>3.729155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Models\n",
    "trainers = {}\n",
    "for col in [\"hazard-category\", \"product-category\", \"hazard\", \"product\"]:\n",
    "    trainers[col] = Trainer(\n",
    "        model=models[col],\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[col][\"train\"],\n",
    "        eval_dataset=datasets[col][\"val\"],\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    print(f\"Training model for: {col}\")\n",
    "    trainers[col].train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6604d9-07f1-43fd-98f5-a39235da112a",
   "metadata": {},
   "source": [
    "## Generating Predictions & Computing Scores\n",
    "\n",
    "Once the model is trained, we need to **evaluate its performance** by generating predictions and computing **F1 scores**.  \n",
    "The evaluation is based on the **hazard category** and **product category** predictions for ST1 and **hazard** and **product** for ST2.\n",
    "\n",
    "---\n",
    "\n",
    "### Generating Predictions\n",
    "We define a function to **obtain predictions** from the trained models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d45f9eb2-028e-43e5-afb7-ebd76b6b75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions & Compute Scores\n",
    "def get_predictions(trainer, dataset):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    return np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    f1_hazards = f1_score(hazards_true, hazards_pred, average='macro')\n",
    "    correct_hazard_mask = hazards_pred == hazards_true\n",
    "    f1_products = f1_score(\n",
    "        np.array(products_true)[correct_hazard_mask], \n",
    "        np.array(products_pred)[correct_hazard_mask], \n",
    "        average='macro'\n",
    "    )\n",
    "    return (f1_hazards + f1_products) / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71af60c-f5e8-4090-bfe8-e93d4ec694b8",
   "metadata": {},
   "source": [
    "## Computing Validation & Test Scores\n",
    "\n",
    "Once the model has been trained and predictions have been generated, we evaluate its performance on both the **validation set** and the **test set**.\n",
    "\n",
    "---\n",
    "\n",
    "### Computing Scores for Validation & Test Sets\n",
    "We use the **`compute_score()`** function to calculate F1 scores for:\n",
    "- **ST1 (hazard-category & product-category)**\n",
    "- **ST2 (hazard & product)**\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b017a06-2b40-411c-9f34-0feb41879079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute Scores\n",
    "st1_val_score = compute_score(y_val[\"hazard-category\"], y_val[\"product-category\"], get_predictions(trainers[\"hazard-category\"], datasets[\"hazard-category\"][\"val\"]), get_predictions(trainers[\"product-category\"], datasets[\"product-category\"][\"val\"]))\n",
    "st2_val_score = compute_score(y_val[\"hazard\"], y_val[\"product\"], get_predictions(trainers[\"hazard\"], datasets[\"hazard\"][\"val\"]), get_predictions(trainers[\"product\"], datasets[\"product\"][\"val\"]))\n",
    "\n",
    "st1_test_score = compute_score(y_test[\"hazard-category\"], y_test[\"product-category\"], get_predictions(trainers[\"hazard-category\"], datasets[\"hazard-category\"][\"test\"]), get_predictions(trainers[\"product-category\"], datasets[\"product-category\"][\"test\"]))\n",
    "st2_test_score = compute_score(y_test[\"hazard\"], y_test[\"product\"], get_predictions(trainers[\"hazard\"], datasets[\"hazard\"][\"test\"]), get_predictions(trainers[\"product\"], datasets[\"product\"][\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0a718-c72c-454e-8338-e4052e461345",
   "metadata": {},
   "source": [
    "## Print Results\n",
    "\n",
    "After evaluating the model on both **validation** and **test** datasets, we print the **final scores** for **ST1 (hazard-category classification)** and **ST2 (hazard vector detection).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6266a2b-4fc2-459b-a569-141bd0b4f1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ST1 Validation Score: 0.7215756440614738\n",
      " ST2 Validation Score: 0.4284686649864853\n",
      " ST1 Test Score: 0.6598065789347851\n",
      " ST2 Test Score: 0.4013321429030964\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "print(f\" ST1 Validation Score: {st1_val_score}\")\n",
    "print(f\" ST2 Validation Score: {st2_val_score}\")\n",
    "print(f\" ST1 Test Score: {st1_test_score}\")\n",
    "print(f\" ST2 Test Score: {st2_test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78befd3d-f19c-4cc0-9eac-4c7fbbc35865",
   "metadata": {},
   "source": [
    "## Save Submission File\n",
    "\n",
    "After generating predictions for both the **test set** and **validation set**, we save them as CSV files for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13dc9a9a-0d1d-4e8d-aeb7-24832a3e3027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_val.csv\n",
      "Submission files created and zipped: `submission.zip`\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Save Predictions to CSV with Proper Labels\n",
    "def save_predictions(trainers, datasets, label_encoders, dataset_type, filename_prefix):\n",
    "    \"\"\"\n",
    "    Generates predictions, converts them back to original labels, and saves them in CSV format.\n",
    "    dataset_type: \"test\" or \"val\" - controls which dataset is used.\n",
    "    \"\"\"\n",
    "    submission_data = {\"ID\": np.arange(1, len(datasets[\"hazard-category\"][dataset_type]) + 1)}  # Add ID column\n",
    "\n",
    "    # Generate predictions for each category and convert back to text labels\n",
    "    for col in [\"hazard-category\", \"product-category\", \"hazard\", \"product\"]:\n",
    "        preds = get_predictions(trainers[col], datasets[col][dataset_type])  \n",
    "        submission_data[col] = label_encoders[col].inverse_transform(preds)  # Convert numbers back to labels\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "    # Save CSV file\n",
    "    filename = f\"{filename_prefix}_{dataset_type}.csv\"\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# Generate and Save Predictions Separately\n",
    "save_predictions(trainers, datasets, label_encoders, \"test\", \"submission\")\n",
    "save_predictions(trainers, datasets, label_encoders, \"val\", \"submission\")\n",
    "\n",
    "# Zip Submission Files\n",
    "with zipfile.ZipFile(\"submission.zip\", \"w\") as zipf:\n",
    "    zipf.write(\"submission_test.csv\")\n",
    "    zipf.write(\"submission_val.csv\")\n",
    "\n",
    "print(\"Submission files created and zipped: `submission.zip`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66217d-0f5c-4f05-9fb3-93a2cd8239d7",
   "metadata": {},
   "source": [
    "# **Final Thoughts**\n",
    "\n",
    "I explored multiple **Machine Learning (ML) and Deep Learning (DL) approaches**, fine-tuning models to **maximize F1 scores while avoiding overfitting**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Models and Approaches Explored**  \n",
    "Throughout the project, I experimented with several models and techniques:\n",
    "\n",
    "### **Traditional Machine Learning Approaches**  \n",
    "1. **Support Vector Machines (SVM)**\n",
    "   - Initial baseline model\n",
    "   - Required feature engineering (TF-IDF)\n",
    "   - Poor performance on imbalanced classes\n",
    "\n",
    "2. **Logistic Regression**\n",
    "   - Fast and interpretable\n",
    "   - Could not capture complex linguistic patterns\n",
    "\n",
    "3. **Random Forest & XGBoost**\n",
    "   - Performed better than linear models\n",
    "   - Struggled with class imbalance and lacked explainability\n",
    "\n",
    "---\n",
    "\n",
    "### **Deep Learning Approaches**  \n",
    "1. **LSTM & Bi-LSTM**\n",
    "   - Tested with Word2Vec & FastText embeddings\n",
    "   - Moderate performance but slow training\n",
    "\n",
    "2. **BERT Variants**  \n",
    "   - `bert-base-uncased` (Baseline Transformer)\n",
    "   - `bert-large-uncased` (Better generalization)\n",
    "   - Did not outperform RoBERTa on validation/test\n",
    "\n",
    "3. **RoBERTa (Final Model)**\n",
    "   - `roberta-base` provided the best balance of speed and accuracy\n",
    "   - Fine-tuned on hazard and product categories\n",
    "   - Applied **class weighting, data augmentation, and SMOTE** to handle class imbalance\n",
    "   - Used **learning rate scheduling (cosine with restarts)**\n",
    "   - Applied **gradient accumulation and mixed-precision training (fp16)**\n",
    "   - **Best validation performance**:  \n",
    "     - **ST1 Validation:** *High accuracy, minimal overfitting*  \n",
    "     - **ST2 Validation:** *Consistent improvements*\n",
    "\n",
    "---\n",
    "\n",
    "## **Challenges and Optimizations**  \n",
    "- **Data Imbalance**:  \n",
    "  - Used `compute_class_weight` for handling imbalance\n",
    "  - Applied **SMOTE** for oversampling product categories  \n",
    "  - Balanced loss functions to avoid bias\n",
    "\n",
    "- **Memory Constraints**:  \n",
    "  - Limited **GPU memory** prevented using larger models (`roberta-large`, `deberta-v3-large`)  \n",
    "  - **Batch size reduced** to **prevent out-of-memory (OOM) errors**  \n",
    "  - Gradient accumulation steps **increased to 8** for **stabilized updates**\n",
    "  - **Mixed-precision (fp16)** reduced memory load\n",
    "\n",
    "- **Hyperparameter Tuning**:  \n",
    "  - Adjusted **learning rate (`1e-5`)**\n",
    "  - Increased **weight decay (`0.07`)**\n",
    "  - Used **cosine learning rate scheduler with warmup**\n",
    "  - Applied **gradient clipping (`max_grad_norm=0.8`)**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insights & Limitations**  \n",
    "- **Final Model (RoBERTa) achieved optimal validation performance**  \n",
    "- **With more GPU memory, we could use larger models for further improvements**    \n",
    "\n",
    "---\n",
    "\n",
    "## **Reference Paper: CICLe - Conformal In-Context Learning for Large-scale Multi-Class Food Risk Classification**  \n",
    "I based some of my ideas on the **CICLe** paper:  \n",
    "*Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren, Stockholm University, 2024.*  \n",
    "Citation: https://arxiv.org/abs/2403.11904\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
